---
title: 'Versatile Inverse Reinforcement Learning via Cumulative Rewards'
collection: publications
permalink: /publication/virl
date: 2021-11-15
venue: 'Workshop on Robot Learning: Self-Supervised and Lifelong Learning @ NeurIPS'
authors: 'Niklas Freymuth, <b> Philipp Becker </b>, Gerhard Neumann'
paperurl: 'https://arxiv.org/pdf/2111.07667'
#slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
#paperurl: 'http://academicpages.github.io/files/paper1.pdf'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

<p>
<h3> Abstract: </h3>

Inverse Reinforcement Learning infers a reward function from expert demonstrations, aiming to encode the behavior and intentions of the expert. Current approaches usually do this with generative and uni-modal models, meaning that they encode a single behavior. In the common setting, where there are various solutions to a problem and the experts show versatile behavior this severely limits the generalization capabilities of these methods. We propose a novel method for Inverse Reinforcement Learning that overcomes these problems by formulating the recovered reward as a sum of iteratively trained discriminators. We show on simulated tasks that our approach is able to recover general, high-quality reward functions and produces policies of the same quality as behavioral cloning approaches designed for versatile behavior.
</p>